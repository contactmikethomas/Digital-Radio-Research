{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Relevant Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import DeleteMany\n",
    "from pymongo.errors import BulkWriteError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/de-admin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: count is deprecated. Use estimated_document_count or count_documents instead. Please note that $where must be replaced by $expr, $near must be replaced by $geoWithin with $center, and $nearSphere must be replaced by $geoWithin with $centerSphere\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "db = pymongo.MongoClient().doc_store\n",
    "total_corpora_articles = db.test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_doc_map = pickle.load(open(\"/home/de-admin/Documents/open data/wikipedia category document map/cat doc map\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_articles = 500\n",
    "minimum_average_words_per_article = 1000\n",
    "minimum_words = minimum_articles * minimum_average_words_per_article\n",
    "maximum_corpora_proportion = 0.001\n",
    "maximum_corpora_articles = total_corpora_articles*maximum_corpora_proportion\n",
    "\n",
    "final_article_corpora = []\n",
    "articles_removed = 0\n",
    "words_removed = 0\n",
    "\n",
    "for (outer_k, outer_v) in cat_doc_map.items():\n",
    "    number = outer_v['number']\n",
    "    words = outer_v['words']\n",
    "    \n",
    "    if number > minimum_articles:\n",
    "        article_dist = outer_v['article_dist']\n",
    "        number_down = 0\n",
    "        words_down = 0\n",
    "        articles_to_use = []\n",
    "        \n",
    "        for (inner_k, inner_v) in article_dist.items():\n",
    "            if int(inner_k) < minimum_average_words_per_article:\n",
    "                number_down = number_down + inner_v[0]\n",
    "                words_down = words_down + (inner_k*inner_v[0])\n",
    "            else:\n",
    "                for article in inner_v[1]:\n",
    "                    articles_to_use.append(article)\n",
    "        \n",
    "        final_number = number - number_down\n",
    "        final_words = words - words_down\n",
    "        \n",
    "        if final_number <= maximum_corpora_articles:\n",
    "            if final_number >= minimum_articles:\n",
    "                if final_words >= minimum_words:\n",
    "                    final_article_corpora.append([outer_k, final_number, final_words, articles_to_use])\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "                pass\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Text Classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split selected corpora into test-training lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Categories:  540 \n",
      "Largest category articles:  5052 \n",
      "Largest category words:  23429096 \n",
      "\n",
      "Total Articles:  556967 \n",
      "Total train articles:  501033 \n",
      "Total test articles:  55934\n"
     ]
    }
   ],
   "source": [
    "# create a dataframe using texts and lables\n",
    "trainDF = pandas.DataFrame(final_article_corpora, columns=[\"label\",\"number articles\",\"number words\",\"titles\"])\n",
    "\n",
    "#sampling the titles per category\n",
    "train_titles = []\n",
    "train_titles_n = []\n",
    "test_titles = []\n",
    "test_titles_n = []\n",
    "\n",
    "for row in trainDF['titles']:\n",
    "    output = model_selection.train_test_split(row, test_size=0.1)\n",
    "    train_titles.append(output[0])\n",
    "    train_titles_n.append(len(output[0]))\n",
    "    test_titles.append(output[1])\n",
    "    test_titles_n.append(len(output[1]))\n",
    "\n",
    "trainDF[\"train titles\"] = train_titles\n",
    "trainDF[\"train titles count\"] = train_titles_n\n",
    "trainDF[\"test titles\"] = test_titles\n",
    "trainDF[\"test titles count\"] = test_titles_n\n",
    "\n",
    "print(\"Total Categories: \",len(trainDF),\"\\nLargest category articles: \",trainDF.iloc[trainDF[\"number articles\"].values.argmax()][\"number articles\"],\"\\nLargest category words: \",trainDF.iloc[trainDF[\"number words\"].values.argmax()][\"number words\"],\"\\n\\nTotal Articles: \", trainDF['number articles'].sum(),\"\\nTotal train articles: \",trainDF['train titles count'].sum(),\"\\nTotal test articles: \",trainDF['test titles count'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Custom Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6977"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum_occurances = 50000\n",
    "maximum_length = 10\n",
    "\n",
    "final_vocab = []\n",
    "\n",
    "for word in db.vocab.find({ \"count\": { \"$gt\": minimum_occurances } }):\n",
    "    if len(word[\"_id\"]) > maximum_length:\n",
    "        pass\n",
    "    else:\n",
    "        final_vocab.append(word[\"_id\"])\n",
    "        \n",
    "len(final_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a generator to call documents one at a time from mongo\n",
    "\n",
    "def make_corpus():\n",
    "    i=0\n",
    "    for row in trainDF[\"train titles\"]:\n",
    "        for title in row:\n",
    "            yield db.test.find_one({'_id': title })[\"text\"]\n",
    "        i = i + 1\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "    \n",
    "corpus = make_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-545e679e82b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcount_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mr'\\w{1,}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \"\"\"\n\u001b[0;32m--> 836\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mj_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Define count vectoriser and fit selected training corpus\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', vocabulary=final_vocab)\n",
    "count_vect.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the training and validation data using count vectorizer object\n",
    "\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
